Tasks to Enable Dual-Model Pipeline

1. Implement Circadian Feature Extraction for XGBoost (Impact: High, Effort=M)
	•	Problem: The XGBoost mood models rely on a 36-dimensional daily feature vector capturing detailed sleep and circadian metrics ￼. Currently, many of these indices (e.g. Sleep Regularity Index, Interdaily Stability, Intradaily Variability, relative amplitude, L5/M10 activity periods) are only computed by an external MATLAB script and not yet implemented in the Python pipeline. This gap means we cannot directly use Apple Health data with the XGBoost models until we reproduce those features in Python ￼.
	•	Implementation Outline: Extend the feature processing code to calculate the full set of circadian and sleep features in Python, mirroring the original research features. The AdvancedFeatureEngineer in advanced_feature_engineering.py already defines placeholders for these metrics – e.g. methods for sleep regularity, IS/IV, RA, L5/M10 ￼ ￼ – but they need to be completed or refined using Apple Health data. Using the daily summaries from SleepAggregator and ActivityAggregator, implement calculations for each required index: Sleep Regularity Index (based on day-to-day consistency of sleep timing), Interdaily Stability (consistency of 24h rhythm across days), Intradaily Variability (fragmentation of daily activity rhythm), Relative Amplitude (difference between most and least active periods), L5 and M10 (lowest 5-hour and highest 10-hour activity levels, ideally using hourly data), as well as sleep timing variability (onset/wake variance) and window percentages (nights <6h or >10h). Populate the AdvancedFeatures dataclass with these values and ensure its to_ml_features() returns a 36-length numpy array as expected by the XGBoost model ￼. Load the pre-trained XGBoost model files (e.g. XGBoost_DE.pkl, XGBoost_ME.pkl in reference_repos/mood_ml ￼) and integrate a prediction function that takes the 36-dimensional feature array and outputs the risk probability for each mood outcome. Key code to modify will include advanced_feature_engineering.py (filling in any stub calculations with real formulas) and possibly a new module or section in the service layer to load the XGBoost pickles and perform model.predict_proba(feature_vector) for each day.
	•	Tests / Acceptance Criteria: Develop unit tests for each new feature calculation using contrived data (e.g. a week of idealized sleep times) to ensure the values make sense (for example, perfect sleep regularity yields index ~100, highly irregular sleep yields much lower). Verify that AdvancedFeatureEngineer.extract_advanced_features produces an AdvancedFeatures object with no nulls and plausible values for all 36 fields on a sample dataset. An integration test (like TestAdvancedFeaturePipeline in the repo) should confirm that calling the full pipeline yields a 36-length feature vector for each day ￼. Specifically, after parsing sample HealthKit data and running FeatureExtractionService.extract_advanced_features, features.to_ml_features() should have length 36 (the test suite asserts this as the Seoul study requirement ￼). Finally, test loading the XGBoost models and generating a prediction: the output should be a probability or risk score without errors. If possible, compare a couple of days against the reference MATLAB output (Index_calculation.m in mood_ml) to ensure the Python features are in reasonable range.
	•	Risks / Mitigations: Risk 1: The Python-computed features may not exactly match the MATLAB version, potentially impacting model accuracy. Mitigation: Start by implementing the most influential features (total sleep, sleep efficiency, sleep timing consistency) and gradually refine the complex circadian metrics. Cross-validate the Python feature outputs against known results or literature definitions, and adjust formulas (or scaling) to closer align with the MATLAB reference ￼. Risk 2: Some required data might be missing or coarse-grained in Apple Health (e.g. exact activity needed for L5/M10). Mitigation: Use proxies from available data (e.g. sedentary vs. active hours as in the current code ￼) and document any assumptions. Ensure the feature extraction handles missing days gracefully (fill with zeros or carry last values) so that the XGBoost model can still produce an output for every day. By implementing these features in Python, we remove the MATLAB dependency and unlock the XGBoost models for the Apple Health pipeline, establishing the core predictive engine.

2. Build Raw Sequence Pipeline for PAT Model (Impact: High, Effort=L)
	•	Problem: The Pretrained Actigraphy Transformer (PAT) requires high-resolution activity time series (minute-level or epoch data over multiple days) as input ￼. Our current pipeline only produces aggregated daily features (e.g. total steps per day) and lacks any mechanism to assemble the raw sequence of activity needed to feed PAT. Without a way to extract and format the Apple HealthKit data into a continuous multi-day sequence, we cannot leverage PAT’s sequence model on the same dataset, leaving a key deep learning component untapped.
	•	Implementation Outline: Develop a data pipeline to retrieve time-stamped activity data and prepare it as a fixed-length sequence tensor for PAT. Apple Health records for steps and energy are time-bounded (each ActivityRecord has a start and end time with a value) – use these to construct a minute-by-minute activity timeline. For example, create a 7-day (10,080-minute) vector for step counts: iterate through each ActivityRecord of type Step Count or Active Energy and distribute its value across the covered minutes (the ActivityAggregator._find_peak_activity_hour already slices step counts by hour as a guide ￼). Fill any minutes with no recorded activity with zero. This yields a per-minute sequence of activity intensity for the week. If memory or data sparsity is a concern, use a 5-minute epoch (2,016-length sequence for a week) as a compromise ￼. Once the sequence is assembled, load the pre-trained PAT model weights provided in reference_repos/Pretrained-Actigraphy-Transformer/model_weights (e.g. PAT-M_29k_weights.h5 for the medium model) ￼. Reconstruct the PAT architecture in code (using TensorFlow/Keras, matching the Transformer layers and patch embeddings as described in the PAT reference). Implement an inference function that takes the activity sequence and feeds it through the PAT model to obtain either an embedding or prediction. Initially, since we may not have a fine-tuned classifier for mood, this could output the PAT’s learned representation or an anomaly score. We might add a simple dense layer on top of PAT’s CLS token output to produce a mood risk estimate, or simply use PAT to flag unusual activity patterns. Integrate this sequence pipeline such that given the same input data, we can produce the PAT input (last N days of minute-level activity) in parallel with the XGBoost features.
	•	Tests / Acceptance Criteria: Write unit tests for the sequence construction: given a few ActivityRecord instances (e.g. 60 minutes with 600 steps total), verify that the resulting minute-by-minute array has the correct length and sum (e.g. 600 total steps distributed, with 10 steps/minute in that interval). Test edge cases like back-to-back records or gaps (the sequence should handle overnight gaps by naturally having zeros when no activity records fall in those minutes). For the model, a smoke test is essential: load the PAT model and run a dummy sequence through it to confirm the model produces an output of expected shape. Because PAT-M is large (~1M parameters) and expects a certain input shape, start with a smaller model or truncate the sequence for testing if needed, ensuring it runs within memory limits. Integration-wise, confirm that for a given real user dataset (if available in tests), the pipeline can generate both the 36 features and the PAT sequence without errors. The acceptance criterion is that we can successfully produce a PAT input tensor from Apple Health data and obtain an output from the PAT model. This sets the stage for using PAT in predictions alongside XGBoost.
	•	Risks / Mitigations: Risk 1: Data resolution mismatch – Apple HealthKit does not provide raw 30Hz accelerometer data (which the original PAT expects) ￼. We only have aggregated metrics like step counts or maybe hourly summaries. Mitigation: Use step count and active energy as proxies for activity intensity in each minute. While this reduces granularity, it still captures overall activity patterns. We can also experiment with downsampling PAT’s expectation (e.g. treat our 1-min data as “patches” directly) or fine-tune PAT on our type of input. Risk 2: Performance – Generating a 10k-length sequence for each prediction and running a transformer on it is computationally heavy, possibly too slow for real-time use. Mitigation: Consider caching and updating the sequence incrementally (e.g. update yesterday’s sequence with today’s new data rather than rebuilding 7 days from scratch each time). Also, we could run the PAT pipeline asynchronously or on a schedule (since circadian patterns change slowly) and use XGBoost for immediate results. Risk 3: Integration complexity – Reimplementing the PAT model architecture and ensuring the weights load correctly can be challenging. Mitigation: Leverage the provided PAT code examples (in reference_repos/Pretrained-Actigraphy-Transformer/Fine-tuning) to build the model class exactly as expected. Start with the simplest configuration (PAT-S or a smaller input window) to verify the end-to-end flow, then scale up to the full model. By addressing these risks, we ensure that the PAT pipeline enriches the system with sequence-based analysis, complementing the day-level XGBoost features.

3. Parallel Inference & Ensemble Integration (Impact: Medium, Effort=M)
	•	Problem: With both the XGBoost feature pipeline and the PAT sequence pipeline in place, we need to combine their outputs for a unified prediction. Right now, there is no orchestrator coordinating multiple models – the system doesn’t know to generate both an XGBoost risk score and a PAT-based insight for the same time period. The research roadmap calls for integrating the transformer model alongside XGBoost in an ensemble ￼, so we must implement a mechanism to run them in parallel and then merge their predictions. Without this, we’d only get isolated outputs and miss the potential accuracy boost from a combined model.
	•	Implementation Outline: Create a higher-level prediction service (e.g. a DualModelPredictor class or an updated FastAPI endpoint) that orchestrates running both models and fusing their results. This component will take the prepared input data (Apple Health records) and do the following in sequence: 1) use the circadian feature pipeline to get the 36-dimensional feature array and XGBoost’s predicted probabilities for each mood outcome, and 2) use the PAT pipeline to get the sequence representation and PAT’s output for the same period. Ensure that both pipelines are aligned on the target date – for example, if PAT uses a 7-day window ending on Day D, use Day D’s features for XGBoost as well. Once both predictions are obtained, combine them. A simple initial approach is to average the risk scores or to define rules (e.g. flag a high risk if either model is confident). For instance, if XGBoost predicts 0.8 mania risk on a certain day and PAT’s analysis (perhaps an anomaly score converted to risk) predicts 0.6, we might average to 0.7 or take the max for safety. The integration logic should be easily adjustable: in the future we might weight XGBoost higher (since it’s the primary model) and PAT lower ￼, or even train a meta-model to combine them. Modify the FastAPI backend (or CLI) to call this orchestrator instead of just the single-model pipeline. For example, in the /predict endpoint handler, after parsing the input data, invoke DualModelPredictor.predict(data) which internally calls both XGBoost and PAT pipelines and returns a merged result. The result could include both models’ scores and the ensemble score. Structurally, this may involve updating the FeatureExtractionService or adding a new service in the domain layer for model inference. Key files to touch will be the API layer (to integrate the new predictor) and a new module (say model_inference.py) implementing the ensemble logic.
	•	Tests / Acceptance Criteria: Because this is a higher-level integration, start with integration tests. Using a small set of synthetic data (or real sample data), simulate a full run: parse the data, run dual-model prediction, and verify that the output contains contributions from both models. For example, if our input includes a week of data up to 2025-07-01, after running the ensemble predictor we expect a result for 2025-07-01 that has an XGBoost risk score (e.g. depression risk 0.3) and a PAT-derived score or flag. If using a simple average, verify mathematically that the ensemble output lies between the two model outputs. Also test edge cases: if the PAT sequence cannot be generated (e.g. not enough data or the user opts out), the system should still return the XGBoost result (perhaps with PAT fields set to null or omitted). Conversely, if sleep data is missing for XGBoost but activity data exists for PAT, the pipeline might still produce a PAT outcome – ensure the code can handle one model producing output while the other doesn’t, without crashing. Acceptance is achieved when a single API call can intake raw HealthKit data and produce an output that clearly reflects both the XGBoost model prediction and the PAT analysis for the same date, demonstrating the ensemble behavior.
	•	Risks / Mitigations: Risk 1: Misaligned model outputs – The XGBoost features are day-specific, while PAT’s sequence spans multiple days. This could lead to confusing results if not aligned (e.g. PAT’s “week risk” being assigned to the wrong day). Mitigation: Clearly define that PAT’s output is anchored to the last day of its input window (or the week overall) and align that with the corresponding XGBoost day. In implementation, when preparing PAT input for Day D, use data from D-6 through D, so that its result is naturally about Day D. Document this assumption and maybe include the date range in the output for clarity. Risk 2: Combining probabilities naively – Simply averaging two model outputs assumes they are equally calibrated, which might not be true (XGBoost probabilities could be well-calibrated, whereas PAT’s output might not even be a probability). Mitigation: As an initial approach, averaging is fine for prototype, but we should monitor performance on any available validation data. We can introduce a weighting factor (e.g. 70% XGBoost, 30% PAT) reflecting our confidence in each ￼. In the future, once we have actual outcome labels, we can train a small logistic regression on top of the two outputs to optimally combine them. Risk 3: Added complexity – Running two models doubles the computation and points of failure. Mitigation: Build in graceful degradation: if PAT pipeline errors out or times out, catch the exception and return the XGBoost result alone (logging the issue for debugging). This ensures the system still provides a prediction rather than failing completely. By carefully coordinating the two models and anticipating pitfalls, we lay a solid foundation for the ensemble approach, which is expected to improve overall prediction robustness and accuracy in the long run ￼.